{"cells": [{"cell_type": "markdown", "id": "f890a57b", "metadata": {}, "source": ["# Prueba técnica – Notebook (Spark + SQL sobre ventas POS)\n", "\n", "Este notebook forma parte de la prueba técnica.\n", "\n", "## Instrucciones\n", "\n", "- NO cambies la estructura del notebook.\n", "- Completa el código en las celdas marcadas con `TODO`.\n", "- Usa siempre `spark.sql(...)` para las preguntas SQL.\n", "- Puedes ejecutar las celdas cuantas veces necesites.\n", "- Al finalizar, guarda el notebook con tus respuestas y súbelo a tu repositorio (GitHub / DevOps) para compartir el enlace en el formulario.\n", "\n", "Las tablas que se usarán son:\n", "\n", "- `products`\n", "- `stores`\n", "- `sales`\n", "- `inventory`\n"]}, {"cell_type": "code", "execution_count": null, "id": "fb3116c8", "metadata": {}, "outputs": [], "source": ["# Configuración inicial de Spark\n", "from pyspark.sql import SparkSession\n", "from pyspark.sql import functions as F\n", "\n", "spark = SparkSession.builder.getOrCreate()\n", "\n", "data_path = \"./data\"  # Ajusta la ruta si es necesario\n"]}, {"cell_type": "markdown", "id": "b57479ad", "metadata": {}, "source": ["## Carga de datasets"]}, {"cell_type": "code", "execution_count": null, "id": "d0698cdd", "metadata": {}, "outputs": [], "source": ["# Cargar los archivos CSV como DataFrames de Spark\n", "\n", "products_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(f\"{data_path}/products.csv\")\n", "stores_df   = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(f\"{data_path}/stores.csv\")\n", "sales_df    = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(f\"{data_path}/sales.csv\")\n", "inventory_df= spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(f\"{data_path}/inventory.csv\")\n", "\n", "print(\"Products:\")\n", "products_df.show(5)\n", "print(\"Stores:\")\n", "stores_df.show(5)\n", "print(\"Sales:\")\n", "sales_df.show(5)\n", "print(\"Inventory:\")\n", "inventory_df.show(5)\n"]}, {"cell_type": "markdown", "id": "e1c77bd8", "metadata": {}, "source": ["## Creación de vistas temporales para SQL"]}, {"cell_type": "code", "execution_count": null, "id": "5bb6acd0", "metadata": {}, "outputs": [], "source": ["# Crear vistas temporales para usarlas con spark.sql\n", "\n", "products_df.createOrReplaceTempView(\"products\")\n", "stores_df.createOrReplaceTempView(\"stores\")\n", "sales_df.createOrReplaceTempView(\"sales\")\n", "inventory_df.createOrReplaceTempView(\"inventory\")\n", "\n", "spark.sql(\"SHOW TABLES\").show()\n"]}, {"cell_type": "markdown", "id": "50bd0abc", "metadata": {}, "source": ["---\n", "# Parte 1 – Retos SQL (usar `spark.sql`)\n", "\n", "Usa únicamente `spark.sql(\"...\")` en las celdas de esta sección.\n", "Puedes crear DataFrames intermedios si lo consideras necesario.\n"]}, {"cell_type": "markdown", "id": "7ded0fc0", "metadata": {}, "source": ["## Reto SQL 1 – Duplicados por `sale_id`\n", "\n", "Encuentra los `sale_id` que estén repetidos en la tabla `sales`, mostrando también la cantidad de repeticiones.\n", "\n", "**Resultado esperado (columnas sugeridas):**\n", "\n", "- `sale_id`\n", "- `veces_repetido`\n", "\n", "Escribe tu consulta usando `spark.sql`.\n"]}, {"cell_type": "code", "execution_count": null, "id": "c318b464", "metadata": {}, "outputs": [], "source": ["# TODO: Escribe tu solución para el Reto SQL 1 usando spark.sql\n", "\n", "sql_reto_1 = \"""-- Escribe aquí tu consulta SQL\n", "-- SELECT ... FROM sales ...\n", """\n", "\n", "df_reto_1 = spark.sql(sql_reto_1)\n", "df_reto_1.show()\n"]}, {"cell_type": "markdown", "id": "83c98591", "metadata": {}, "source": ["## Reto SQL 2 – Ventas por tienda y fecha con función de ventana\n", "\n", "Para cada combinación de `store_id` y `sale_date` calcula:\n", "\n", "- El total vendido del día (`total_dia` = SUM(quantity * unit_price))\n", "- El ranking de la tienda dentro de esa fecha según el `total_dia` (1 = mayor venta)\n", "\n", "**Resultado esperado (columnas sugeridas):**\n", "\n", "- `sale_date`\n", "- `store_id`\n", "- `total_dia`\n", "- `ranking_en_fecha`\n", "\n", "Escribe tu consulta usando funciones de ventana en SQL.\n"]}, {"cell_type": "code", "execution_count": null, "id": "23f51f5a", "metadata": {}, "outputs": [], "source": ["# TODO: Escribe tu solución para el Reto SQL 2 usando spark.sql\n", "\n", "sql_reto_2 = \"""-- Escribe aquí tu consulta SQL con funciones de ventana\n", "-- Sugerencia: usar SUM(...) OVER(...) y RANK() OVER(...)\n", """\n", "\n", "df_reto_2 = spark.sql(sql_reto_2)\n", "df_reto_2.show()\n"]}, {"cell_type": "markdown", "id": "e90fa9de", "metadata": {}, "source": ["## Reto SQL 3 – Top 3 productos por región según ventas totales\n", "\n", "Usando las tablas `sales`, `products` y `stores`, obtén el **TOP 3 productos por región** según el valor total vendido:\n", "\n", "- Total vendido = SUM(quantity * unit_price)\n", "\n", "**Resultado esperado (columnas sugeridas):**\n", "\n", "- `region`\n", "- `product_id`\n", "- `name` (nombre del producto)\n", "- `total_vendido`\n", "- `ranking_region` (1 a 3)\n", "\n", "Escribe tu consulta usando SQL con funciones de ventana.\n"]}, {"cell_type": "code", "execution_count": null, "id": "49f4db08", "metadata": {}, "outputs": [], "source": ["# TODO: Escribe tu solución para el Reto SQL 3 usando spark.sql\n", "\n", "sql_reto_3 = \"""-- Escribe aquí tu consulta SQL para el TOP 3 por región\n", "-- Recuerda hacer JOIN entre sales, products y stores\n", """\n", "\n", "df_reto_3 = spark.sql(sql_reto_3)\n", "df_reto_3.show()\n"]}, {"cell_type": "markdown", "id": "35becee6", "metadata": {}, "source": ["---\n", "# Parte 2 – Retos Spark (DataFrames / PySpark)\n", "\n", "En esta sección debes usar la API de DataFrames de PySpark.\n"]}, {"cell_type": "markdown", "id": "b3d9d782", "metadata": {}, "source": ["## Reto Spark 1 – Tabla Silver de ventas por tienda y producto\n", "\n", "Crea una tabla \"Silver\" agregada con el total vendido por `store_id` y `product_id`.\n", "\n", "- Total vendido = SUM(quantity * unit_price)\n", "- Usa el DataFrame `sales_df`\n", "- El resultado debe contener:\n", "  - `store_id`\n", "  - `product_id`\n", "  - `total_vendido`\n", "\n", "Guarda el resultado en formato Delta en la ruta `./delta/sales_silver` (si tu entorno lo permite).\n"]}, {"cell_type": "code", "execution_count": null, "id": "8d69b813", "metadata": {}, "outputs": [], "source": ["# TODO: Escribe tu solución para el Reto Spark 1 usando la API de DataFrames\n", "\n", "# Ejemplo de estructura (puedes cambiarla si quieres):\n", "# 1. Crear columna total_línea = quantity * unit_price\n", "# 2. Agrupar por store_id, product_id\n", "# 3. Guardar en formato Delta\n", "\n", "# silver_df = (\n", "#     sales_df\n", "#     .withColumn(\"total_linea\", F.col(\"quantity\") * F.col(\"unit_price\"))\n", "#     .groupBy(\"store_id\", \"product_id\")\n", "#     .agg(F.sum(\"total_linea\").alias(\"total_vendido\"))\n", "# )\n", "\n", "# silver_df.show()\n", "\n", "# Opcional: guardar como Delta si está disponible\n", "# silver_df.write.format(\"delta\").mode(\"overwrite\").save(\"./delta/sales_silver\")\n"]}, {"cell_type": "markdown", "id": "03cdfe15", "metadata": {}, "source": ["## Reto Spark 2 – UDF para formatear nombres de productos\n", "\n", "Crea una UDF en PySpark que reciba el nombre de un producto (`name`) y retorne el texto:\n", "\n", "`\"SKU-\" + nombre_en_mayúsculas`\n", "\n", "Aplica esta UDF sobre el DataFrame `products_df` y muestra al menos 5 filas con las columnas:\n", "\n", "- `product_id`\n", "- `name`\n", "- `name_formateado`\n"]}, {"cell_type": "code", "execution_count": null, "id": "4a266693", "metadata": {}, "outputs": [], "source": ["# TODO: Escribe tu solución para el Reto Spark 2 creando y usando una UDF\n", "\n", "# from pyspark.sql.types import StringType\n", "\n", "# def formatear_nombre(name: str) -> str:\n", "#     # Implementa aquí la lógica de la UDF\n", "#     return ...\n", "\n", "# formatear_nombre_udf = F.udf(formatear_nombre, StringType())\n", "\n", "# products_formatted_df = products_df.withColumn(\n", "#     \"name_formateado\",\n", "#     formatear_nombre_udf(F.col(\"name\"))\n", "# )\n", "\n", "# products_formatted_df.show(5)\n"]}, {"cell_type": "markdown", "id": "a5df5473", "metadata": {}, "source": ["---\n", "## Fin del notebook\n", "\n", "Guarda este archivo con tus respuestas y súbelo a tu repositorio para compartirlo en el formulario de la prueba.\n"]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 5}